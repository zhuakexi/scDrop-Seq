#' ---
#' title:  create_summary_stats.R
#' author: Sebastian Mueller (sebm_at_posteo.de)
#' date:   2019-03-04
#' desc: Creating various summary statistics for barcodes (pre and post filtered)

# o   A delimited file containing information for each STAMP (before cut-off) on number of UMIs, number of Genes detected/captured and the number of NGS-reads
# o   A separate delimited file containing information for each STAMP (after cut-off) on number of UMIs, number of Genes detected/captured and the number of NGS-reads
# Example of the format could be as follows:
# STAMP id          Number of NGS-reads       Number of UMIs       Number of Genes Detected
# STAMP1           1000000                           50000                       6000
#' ---

#------------------------------------ for debuging:
# add the following line in config.yaml (without the #)
# DEBUG: True
# This will create R objects in the debug directory containing the snakemake object
#  R object that can be loaded into a custom R session as below:
# load("debug/snakemake_create_summary_stats.rdata")
# load(file="debug/R_image_create_summary_stats.rdata")

debug_flag <- FALSE
# check if DEBUG flag is set
if (snakemake@config$DEBUG) {
  debug_flag <- TRUE
  message("In debug mode: saving R objects to inspect later")
  path_debug <- file.path(snakemake@config$LOCAL$results, "debug")
  dir.create(path_debug, showWarnings = FALSE)
  save(snakemake, file = file.path(path_debug, "create_summary_stats_snakemake.rdata"))
}
#### /debug

library(dplyr) # Dataframe manipulation
library(Matrix) # Sparse matrices
library(stringr)
library(RColorBrewer)
library(devtools)
library(Seurat)
library(plotly)

samples <- snakemake@params$sample_names
batches <- snakemake@params$batches


# importing Seurat object

seuratobj <- readRDS(file  = file.path(snakemake@input$R_objects))
meta.data <- seuratobj@meta.data

# subset only highest stamps as set in config.yaml
# this is necessary since there are more stamps selected as a safty margin which now have to be taken out again to calculate stats.

meta.data.sub <- meta.data %>%
  group_by(orig.ident) %>%
  arrange(desc(nCounts)) %>%
  slice(1:expected_cells[1]) %>% # makes sure only # expected cell are kept
  as.data.frame()

gini_index <- function (x, weights = rep(1, length = length(x))) {
    ox      <- order(x)
    x       <- x[ox]
    weights <- weights[ox] / sum(weights)
    p       <- cumsum(weights)
    nu      <- cumsum(weights * x)
    n       <- length(nu)
    nu      <- nu/nu[n]
    sum(nu[-1] * p[-n]) - sum(nu[-n] * p[-1])
}

#------------------------------------ post-filter-stats
# stats only based after keeping most abundant `expected-cells`
# taken out from seurat object generated in violine_plot rule.

# median calculator

stats_post <- meta.data.sub %>%
  group_by(orig.ident) %>%
  summarise(
    Total_nb_reads                 = sum(nCounts),
    Nb_STAMPS                      = mean(expected_cells), # should be all the same anyway..
    Median_reads_per_STAMP         = round(median(nCounts), 2),
    Mean_reads_per_STAMP           = round(mean(nCounts), 2),
    Total_nb_UMIs                  = sum(nUMI),
    Median_UMIs_per_STAMP          = round(median(nUMI), 2),
    Mean_UMIs_per_STAMP            = round(mean(nUMI), 2),
    Mean_UMIs_per_Gene             = round(mean(umi.per.gene), 2),
    Median_number_genes_per_STAMP  = round(median(nGene), 2),
    Mean_number_genes_per_STAMP    = round(mean(nGene), 2),
    Mean_Ribo_pct                  = round(100 * mean(pct.Ribo), 2),
    Mean_Mito_pct                  = round(100 * mean(pct.mito), 2),
    Mean_Count_per_UMI             = round(sum(nCounts) / sum(nUMI), 2),
    Read_length                    = mean(read_length), # should be all the same anyway..
    Number_barcodes_used_for_debug = n()
  ) %>%
  as.data.frame()

row.names(stats_post) <- stats_post$orig.ident



# highest, lowest count/UMI Stamp
# pre STAMP stats

# hist out goes into knee plots
# 'results/logs/{sample}_hist_out_cell.txt'
# """export _JAVA_OPTIONS=-Djava.io.tmpdir={params.temp_directory} && BAMTagHistogram -m {params.memory}\
# TAG=XC\
# https://hpc.nih.gov/apps/dropseq.html
# there is not hint in documentation on any filtering (only read quality)

#------------------------------------ pre-filter-stats
# calculating statistics based on barcodes before thresholding it (i.e. keeping the most abudant barcodes based on `expected cells`)
# This is based on 'logs/{sample}_hist_out_cell.txt' generated by `BAMTagHistogram` from dropseq-tools
# TOOO: The documentation only mentions duplicate and quality filter. But it seems do more filtering since most barcodes are expected to have only one read assinged but there are usually more. Find out.
# https://hpc.nih.gov/apps/dropseq.html

# TODO: Nr UMI for pre filter

stats_pre <- data.frame(matrix(nrow = length(samples), ncol = 10))
colnames(stats_pre) <- c(
  "Sample",
  "Batch",
  "Total_raw_reads",
  "Nr_barcodes_total",
  "Nr_barcodes_more_than_1_reads",
  "Nr_barcodes_more_than_10_reads",
  "percentile99",
  "percentile95",
  "percentile50",
  "Gini-index"
)

stats_pre[, "Sample"] <- samples
stats_pre[, "Batch"]  <- batches


for (i in 1:length(samples)) {
  # importing 'logs/{sample}_hist_out_cell.txt'
  hist_out <- read.table(
    file = snakemake@input$hist_cell[i],
    header = FALSE, stringsAsFactors = FALSE
  )
  mysample <- samples[i]
  reads <- hist_out$V1
  barcodes <- hist_out$V2
  # calculations on reads
  # total reads are not sum(reads)! Needs to be taken from
  # results/logs/cutadapt/sample1_R1.qc.txt
  #
  # read in full text file "sample_R2.qc.txt"
  filedump <- readLines(snakemake@input$R2qc[i])
  # subset line matching a pattern
  total_reads <- filedump[grep("Total reads processed:", filedump)] %>% #extract line
    str_extract("[0-9,]+") %>% # extract number from line
    str_replace_all(",", "") %>% # delete comma for subsequent numeric casting
    as.numeric()
  reads_cumsum      <- cumsum(reads)
  reads_cumsum_perc <- (reads_cumsum / sum(reads))
  # reporting stats
  stats_pre[i, "Total_raw_reads"]                <- total_reads
  stats_pre[i, "Reads_assigned_to_expected_STAMPs"] <- sum(reads[1:stats_post$Nb_STAMPS[i]])
  stats_pre[i, "Nr_barcodes_total"]              <- length(barcodes)
  stats_pre[i, "percentile99"]                   <- which.min(reads_cumsum_perc < 0.99)
  stats_pre[i, "percentile95"]                   <- which.min(reads_cumsum_perc < 0.95)
  stats_pre[i, "percentile50"]                   <- which.min(reads_cumsum_perc < 0.50)
  stats_pre[i, "Nr_barcodes_more_than_1_reads"]  <- sum(reads > 1)
  stats_pre[i, "Nr_barcodes_more_than_10_reads"] <- sum(reads > 10)
  stats_pre[i, "Gini-index"]                     <- round(gini_index(reads), 2)
  expected_cells <- as.numeric(filter(stats_post, orig.ident==mysample) %>% select(Nb_STAMPS))
  # % of reads left after applying expected_cells cuttoff
  stats_post[mysample, "Pct_reads_after_filter_expected_cells"] <-
    round(100 * (
                 reads_cumsum[expected_cells] / sum(reads)
    ), 2)
  # % of reads left after applying all filters including mapping etc.
  # Thats the effecive usable reads of the sequencing run
  stats_post[mysample, "Pct_reads_after_filter_everything"] <-
    round(100 * (
                 filter(stats_post, orig.ident==mysample) %>% select(Total_nb_reads) /
                 stats_pre [i, "Total_raw_reads"]), 2
         )
}

stats_pre <- stats_pre %>%
  arrange(Sample)

stats_post <- stats_post %>%
  arrange((orig.ident))

# output
write.csv(stats_pre,  file.path(snakemake@output$stats_pre))
write.csv(stats_post, file.path(snakemake@output$stats_post)) # writes table for excel

if (debug_flag) {
  save.image(file = file.path(path_debug, "create_summary_stats_workspace.rdata"))
}
